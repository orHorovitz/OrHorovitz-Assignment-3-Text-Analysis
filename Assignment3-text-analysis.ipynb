{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - Text Analysis\n",
    "An explanation this assignment could be found in the .pdf explanation document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Materials to review for this assignment\n",
    "<h4>From Moodle:</h4> \n",
    "<h5><u>Review the notebooks regarding the following python topics</u>:</h5>\n",
    "<div class=\"alert alert-info\">\n",
    "&#x2714; <b>Working with strings</b> (tutorial notebook)<br/>\n",
    "&#x2714; <b>Text Analysis</b> (tutorial notebook)<br/>\n",
    "&#x2714; <b>Hebrew text analysis tools (tokenizer, wordnet)</b> (moodle example)<br/>\n",
    "&#x2714; <b>(brief review) All previous notebooks</b><br/>\n",
    "</div> \n",
    "<h5><u>Review the presentations regarding the following topics</u>:</h5>\n",
    "<div class=\"alert alert-info\">\n",
    "&#x2714; <b>Text Analysis</b> (lecture presentation)<br/>\n",
    "&#x2714; <b>(brief review) All other presentations</b><br/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personal Details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Details Student 1: Or Horovitz , 316283944\n",
    "\n",
    "# Details Student 2: Avishay Elrom , 207946591\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preceding Step - import modules (packages)\n",
    "This step is necessary in order to use external modules (packages). <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# --------------------------------------\n",
    "\n",
    "\n",
    "# --------------------------------------\n",
    "# ------------- visualizations:\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "# --------------------------------------\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "import sklearn\n",
    "from sklearn import preprocessing, metrics, pipeline, model_selection, feature_extraction \n",
    "from sklearn import naive_bayes, linear_model, svm, neural_network, neighbors, tree\n",
    "from sklearn import decomposition, cluster\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score, silhouette_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import Perceptron, SGDClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_selection import SelectKBest#הוספה \n",
    "from sklearn.feature_selection import mutual_info_classif##\n",
    "from sklearn.preprocessing import MaxAbsScaler##\n",
    "from sklearn.model_selection import RepeatedKFold##\n",
    "from sklearn.metrics import make_scorer##\n",
    "# ---------------------------------------\n",
    "\n",
    "\n",
    "# ----------------- output and visualizations: \n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.simplefilter(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "# show several prints in one cell. This will allow us to condence every trick in one cell.\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline\n",
    "pd.pandas.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "# ---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text analysis and String manipulation imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "# --------- Text analysis and Hebrew text analysis imports:\n",
    "# vectorizers:\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# regular expressions:\n",
    "import re\n",
    "# --------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) Hebrew text analysis - WordNet (for Hebrew)\n",
    "Note: the WordNet is not a must"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (optional) Only if you didn't install Wordnet (for Hebrew) use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word net installation:\n",
    "\n",
    "# unmark if you want to use and need to install\n",
    "# !pip install wn\n",
    "# !python -m wn download omw-he:1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word net import:\n",
    "\n",
    "# unmark if you want to use:\n",
    "# import wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) Hebrew text analysis - hebrew_tokenizer (Tokenizer for Hebrew)\n",
    "Note: the hebrew_tokenizer is not a must"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (optional) Only if you didn't install hebrew_tokenizer use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hebrew tokenizer installation:\n",
    "\n",
    "# unmark if you want to use and need to install:\n",
    "# !pip install hebrew_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hebrew tokenizer import:\n",
    "\n",
    "# unmark if you want to use:\n",
    "# import hebrew_tokenizer as ht"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading input files\n",
    "Reading input files for train annotated corpus (raw text data) corpus and for the test corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = 'annotated_corpus_for_train.csv'\n",
    "test_filename  = 'corpus_for_test.csv'\n",
    "df_train = pd.read_csv(train_filename, index_col=None, encoding='utf-8')\n",
    "df_test  = pd.read_csv(test_filename, index_col=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>כשחבר הזמין אותי לחול, לא באמת חשבתי שזה יקרה,...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>לפני שהתגייסתי לצבא עשיתי כל מני מיונים ליחידו...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>מאז שהתחילו הלימודים חלומו של כל סטודנט זה הפנ...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>כשהייתי ילד, מטוסים היה הדבר שהכי ריתק אותי. ב...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>‏הייתי מדריכה בכפר נוער ומתאם הכפר היינו צריכי...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>לפני כ3 חודשים טסתי לרומא למשך שבוע. טסתי במטו...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>אני כבר שנתיים נשוי והשנה אני ואישתי סוף סוף י...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>השנה התחלנו שיפוץ בדירה שלנו בתל אביב. הדירה ה...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               story gender\n",
       "0  כשחבר הזמין אותי לחול, לא באמת חשבתי שזה יקרה,...      m\n",
       "1  לפני שהתגייסתי לצבא עשיתי כל מני מיונים ליחידו...      m\n",
       "2  מאז שהתחילו הלימודים חלומו של כל סטודנט זה הפנ...      f\n",
       "3  כשהייתי ילד, מטוסים היה הדבר שהכי ריתק אותי. ב...      m\n",
       "4  ‏הייתי מדריכה בכפר נוער ומתאם הכפר היינו צריכי...      f\n",
       "5  לפני כ3 חודשים טסתי לרומא למשך שבוע. טסתי במטו...      f\n",
       "6  אני כבר שנתיים נשוי והשנה אני ואישתי סוף סוף י...      m\n",
       "7  השנה התחלנו שיפוץ בדירה שלנו בתל אביב. הדירה ה...      f"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(753, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(8)\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_example_id</th>\n",
       "      <th>story</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>כל קיץ אני והמשפחה נוסעים לארצות הברית לוס אנג...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>הגעתי לשירות המדינה אחרי שנתיים כפעיל בתנועת \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>אחת האהבות הגדולות שלי אלו הכלבים שלי ושל אישת...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_example_id                                              story\n",
       "0                0  כל קיץ אני והמשפחה נוסעים לארצות הברית לוס אנג...\n",
       "1                1  הגעתי לשירות המדינה אחרי שנתיים כפעיל בתנועת \"...\n",
       "2                2  אחת האהבות הגדולות שלי אלו הכלבים שלי ושל אישת..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(323, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(3)\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your implementation:\n",
    "Write your code solution in the following code-cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df_clean:\n",
    "This code defines a function named df_clean that aimed at cleaning text data in a DataFrame column named \"story.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def df_clean(df):\n",
    "    for i in df.index:\n",
    "        df.loc[i, \"story\"] = re.sub(r'\\d+', '', df.loc[i, \"story\"])\n",
    "        df.loc[i, \"story\"] = re.sub(r'[^\\w\\s]', '', df.loc[i, \"story\"])\n",
    "        df.loc[i, \"story\"] = re.sub(r'\\s+', ' ', df.loc[i, \"story\"])\n",
    "        df.loc[i, \"story\"] = df.loc[i, \"story\"].strip()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train_cleaned = df_clean(df_train.copy())  # Make a copy to avoid modifying the original DataFrame\n",
    "df_test_cleaned = df_clean(df_test.copy())    # Make a copy to avoid modifying the original DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train_test_split_func:\n",
    "This code defines a function named train_test_split_func that designed to split a DataFrame into training and test sets. It takes a DataFrame as input and returns the training and test data as well as their corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_test_split_func(df):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df[\"story\"], df[\"gender\"], test_size=0.2, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split_func(df_train_cleaned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer_func:\n",
    "This code defines a function named TfidfVectorizer_func that aims to convert text data into TF-IDF (Term Frequency-Inverse Document Frequency) vector representations. The function takes two sets of text data (X_train and X_test) along with optional parameters to customize the TF-IDF transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TfidfVectorizer_func(X_train, X_test, ngram_range = (1,1), min_df = 5):\n",
    "    count_idf = TfidfVectorizer(min_df = min_df, ngram_range=ngram_range) #creating an object idfvectorizer\n",
    "    X_train_idf = count_idf.fit_transform(X_train)\n",
    "    X_test_idf = count_idf.transform(X_test)\n",
    "    return X_train_idf, X_test_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select_best_features:\n",
    "This code defines a function named select_best_features that aims to perform feature selection using the mutual information criterion. The function takes the training and test feature data (X_train and X_test), corresponding target data (y_train), and an optional parameter k which specifies the number of best features to select. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_features(X_train, X_test, y_train, k=1000):\n",
    "    select_best = SelectKBest(mutual_info_classif, k=k)  # Create a SelectKBest object\n",
    "    select_best.fit(X_train, y_train)  # Fit the feature selector on training data\n",
    "    \n",
    "    X_train_select = select_best.transform(X_train)  # Transform training data to selected features\n",
    "    X_test_select = select_best.transform(X_test)    # Transform test data to selected features\n",
    "    return X_train_select, X_test_select\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preform_MaxAbsScale:\n",
    "This code defines a function named preform_MinMaxScale that aims to perform Min-Max scaling on the selected features. The function takes the selected training and test feature data (X_train_select and X_test_select) and applies Min-Max scaling to transform the data into a specified range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preform_MaxAbsScale(X_train_select, X_test_select):\n",
    "    scaler = MaxAbsScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_select)\n",
    "    X_test_scaled = scaler.transform(X_test_select)\n",
    "    return X_train_scaled, X_test_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_idf, X_test_idf = TfidfVectorizer_func(X_train, X_test)\n",
    "X_train_select, X_test_select = select_best_features(X_train_idf, X_test_idf, y_train)\n",
    "X_train_scale, X_test_scale = preform_MaxAbsScale(X_train_select, X_test_select)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find_best_params_grid_search:\n",
    "This code defines a function named find_best_params_grid_search that is used for performing a grid search to find the best hyperparameters for a given model using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_params_grid_search(model_clf, model_parameters, X_train, y_train):\n",
    "    grid_search = GridSearchCV(model_clf, model_parameters, cv = 5, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    return grid_search.best_params_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODELS:\n",
    "This code demonstrates how to use the find_best_params_grid_search function to search for the best hyperparameters for various machine learning models using cross-validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear_svc_parmas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1, 'dual': False, 'penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "Linear_svc_parmas = [{'C': [0.01, 0.1, 1, 10, 100], 'penalty': [None, 'l1', 'l2'], 'dual': [False]}]\n",
    "best_params_linear_svc = find_best_params_grid_search(LinearSVC(), Linear_svc_parmas, X_train_scale, y_train)\n",
    "print(best_params_linear_svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron_params:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.0001, 'penalty': None}\n"
     ]
    }
   ],
   "source": [
    "Perceptron_params = [{'alpha': [0.0001, 0.05], 'penalty': [None, 'l2', 'l1', 'elasticnet']}]\n",
    "best_params_perceptron = find_best_params_grid_search(Perceptron(), Perceptron_params, X_train_scale, y_train)\n",
    "print(best_params_perceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD_parmas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.05, 'loss': 'modified_huber', 'penalty': 'l2'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SGD_parmas = [{'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
    "               'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "               'alpha': [0.0001, 0.05]}]\n",
    "best_params_SGD = find_best_params_grid_search(SGDClassifier(), SGD_parmas, X_train_scale, y_train)\n",
    "best_params_SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultinomailNB_parmas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 1, 'class_prior': None, 'fit_prior': True}\n"
     ]
    }
   ],
   "source": [
    "MultinomailNB_parmas = [{'alpha': [0.01, 0.1, 0.5, 1], 'fit_prior': [True, False], 'class_prior': [None, [0.5, 0.5], [0.3, 0.7]]}]\n",
    "best_params_MultinomialNB = find_best_params_grid_search(MultinomialNB(), MultinomailNB_parmas, X_train_scale, y_train)\n",
    "print(best_params_MultinomialNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP_params:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "MLP_params = [{'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)], 'activation': ['tanh', 'relu'],\n",
    "               'solver': ['sgd', 'adam'], 'alpha': [0.0001, 0.05], 'learning_rate': ['constant','adaptive']}]\n",
    "best_params_MLP = find_best_params_grid_search(MLPClassifier(),MLP_params, X_train_scale, y_train)\n",
    "print(best_params_MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Linear_model = LinearSVC(**best_params_linear_svc)  #linear model\n",
    "Perceptron_model = Perceptron(**best_params_perceptron) #perceptron model\n",
    "SGD_model = SGDClassifier(**best_params_SGD) #SGD model\n",
    "MultinomialNB_model = MultinomialNB(**best_params_MultinomialNB) #Multinomial naive bayse model\n",
    "MLP_model = MLPClassifier(**best_params_MLP) #MLP model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit_predict_evaluate:\n",
    "\n",
    "This code defines a function named fit_predict_evaluate that is intended to train a machine learning model, perform cross-validation, and evaluate its performance using F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_predict_evaluate(model_name, X_train, X_test, y_train, y_test):\n",
    "    model_trained = model_name.fit(X_train, y_train)  # Fitting the model on the training data\n",
    "    \n",
    "    cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=42)  # Cross-validation settings\n",
    "    f1_scorer = make_scorer(f1_score, average='micro')  # F1 scorer with micro-average\n",
    "    scores = cross_val_score(model_trained, X_test, y_test, scoring=f1_scorer, cv=cv, n_jobs=-1)\n",
    "    \n",
    "    y_pred = model_trained.predict(X_test)  # Predicting labels on test data\n",
    "    \n",
    "    f1_male = f1_score(y_test, y_pred, pos_label=\"m\")  # F1 score for male class\n",
    "    f1_female = f1_score(y_test, y_pred, pos_label=\"f\")  # F1 score for female class\n",
    "    f1_average = (f1_male + f1_female) / 2  # Average F1 score\n",
    "    \n",
    "    return model_trained, f1_average\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESULTS:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score Linear_svc:\n",
      "0.6643378850428707\n",
      "f1_score perceptron:\n",
      "0.7136083451872925\n",
      "f1_score SGD:\n",
      "0.6283076923076923\n",
      "f1_score MultinomialNB:\n",
      "0.6864303616183316\n",
      "f1_score MLP:\n",
      "0.6391875746714457\n"
     ]
    }
   ],
   "source": [
    "linear_trained, f1_average_linear = fit_predict_evaluate(Linear_model, X_train_scale, X_test_scale, y_train, y_test)\n",
    "print(\"f1_score Linear_svc:\")\n",
    "print(f1_average_linear)\n",
    "\n",
    "perceptron_trained, f1_average_perceptron = fit_predict_evaluate(Perceptron_model, X_train_scale, X_test_scale, y_train, y_test)\n",
    "print(\"f1_score perceptron:\")\n",
    "print(f1_average_perceptron)\n",
    "\n",
    "SGD_trained, f1_average_SGD = fit_predict_evaluate(SGD_model, X_train_scale, X_test_scale, y_train, y_test)\n",
    "print(\"f1_score SGD:\")\n",
    "print(f1_average_SGD)\n",
    "\n",
    "MultinomialNB_trained, f1_average_MultinomialNB = fit_predict_evaluate(MultinomialNB_model, X_train_scale, X_test_scale, y_train, y_test)\n",
    "print(\"f1_score MultinomialNB:\")\n",
    "print(f1_average_MultinomialNB)\n",
    "\n",
    "MLP_trained, f1_average_MLP = fit_predict_evaluate(MLP_model, X_train_scale, X_test_scale, y_train, y_test)\n",
    "print(\"f1_score MLP:\")\n",
    "print(f1_average_MLP)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer_func_test:\n",
    "This code defines a function named TfidfVectorizer_func_test that performs TF-IDF vectorization on a given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TfidfVectorizer_func_test(X, ngram_range = (1,1), min_df = 5):\n",
    "    count_idf = TfidfVectorizer(min_df = min_df, ngram_range=ngram_range) #creating an object idfvectorizer\n",
    "    X = count_idf.fit_transform(X)\n",
    "    return X # return the new dataset with the idf values for each word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select_best_features_test:\n",
    "This code defines a function named select_best_features_test that performs feature selection using the SelectKBest method with mutual information criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_features_test(X, k=1000):\n",
    "    select_best = SelectKBest(mutual_info_classif, k=k)\n",
    "    X_test_selected = select_best.fit_transform(X, np.zeros(X.shape[0]))\n",
    "    return X_test_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preform_MinMaxScale_test:\n",
    "This code defines a function named preform_MinMaxScale_test that performs Min-Max scaling on the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preform_MinMaxScale_test(X_train_select):\n",
    "    scale = MinMaxScaler()\n",
    "    X_train_scale = scale.fit_transform(X_train_select.toarray())\n",
    "    return X_train_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test only:\n",
      "     test_example_id predicted_category\n",
      "0                  0                  f\n",
      "1                  1                  m\n",
      "2                  2                  m\n",
      "3                  3                  f\n",
      "4                  4                  m\n",
      "..               ...                ...\n",
      "318              318                  m\n",
      "319              319                  f\n",
      "320              320                  f\n",
      "321              321                  m\n",
      "322              322                  f\n",
      "\n",
      "[323 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "X_df_test = df_test[\"story\"]  # Extracting only the stories from the test dataset\n",
    "X_df_test = TfidfVectorizer_func_test(X_df_test)  # Applying TF-IDF vectorization\n",
    "X_df_test = select_best_features_test(X_df_test)  # Applying feature selection\n",
    "X_df_test_scaled = preform_MinMaxScale_test(X_df_test)  # Applying Min-Max scaling\n",
    "\n",
    "y_prediction_test = perceptron_trained.predict(X_df_test_scaled)  # Predicting on the test data\n",
    "\n",
    "# Creating a DataFrame with test example IDs and predicted categories\n",
    "df_text_exmp = df_test[\"test_example_id\"]\n",
    "df_predicted = pd.DataFrame({\"test_example_id\": df_text_exmp.tolist(),\n",
    "                             \"predicted_category\": y_prediction_test.tolist()})\n",
    "\n",
    "print(\"Test only:\")\n",
    "print(df_predicted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save output to csv (optional)\n",
    "After you're done save your output to the 'classification_results.csv' csv file.<br/>\n",
    "We assume that the dataframe with your results contain the following columns:\n",
    "* column 1 (left column): 'test_example_id'  - the same id associated to each of the test stories to be predicted.\n",
    "* column 2 (right column): 'predicted_category' - the predicted gender value for each of the associated story. \n",
    "\n",
    "Assuming your predicted values are in the `df_predicted` dataframe, you should save you're results as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predicted.to_csv('classification_results.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
